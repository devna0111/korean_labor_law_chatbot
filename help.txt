# docker-compose.yml
---
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.33.0
    ports:
    - 8080:8080
    - 50051:50051
    volumes:
    - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_MODULES: 'text2vec-ollama,generative-ollama'
      CLUSTER_HOSTNAME: 'node1'
volumes:
  weaviate_data:
...

# 위 파일을 생성후
docker-compose up -d

# weaviate 체킹용 라이브러리
pip install weaviate-client

# API 엔드포인트 테스트 
curl http://localhost:8080/v1/meta

# Qwen2.5-1.5B-Instruct 모델 다운로드
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct \
  --local-dir ./models/qwen2.5-1.5b \
  --local-dir-use-symlinks False

# vLLM 서버 시작 (새 터미널)
python -m vllm.entrypoints.openai.api_server \
  --model ./models/qwen2.5-1.5b \
  --dtype half \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.85 \
  --port 8000


# 모든 서비스 테스트

# 1. Ollama 서비스
curl http://localhost:11434/api/tags

# 2. Weaviate
curl http://localhost:8080/v1/meta

# 3. vLLM
curl http://localhost:8000/v1/models

# 4. Python 패키지
python -c "import langchain, weaviate, vllm; print('OK')"